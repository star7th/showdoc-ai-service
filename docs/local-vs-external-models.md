# 本地模型与外部大模型的区别和联系

## 一、概述

ShowDoc AI 服务采用了**混合架构**，同时使用本地模型和外部大模型，各自发挥优势：

- **本地模型**：用于文档向量化（Embedding），将文档转换为向量存储
- **外部大模型**：用于生成回答（LLM），基于检索到的文档内容生成回答

这种设计在**性能、成本、隐私和准确度**之间取得了最佳平衡。

---

## 二、两种模型在本项目中的使用

### 1. Embedding 模型（文档向量化）

**作用**：将文档内容转换为向量，用于语义检索

**当前配置**：
- **默认使用本地模型**：`BAAI/bge-base-zh-v1.5`
- **可选外部 API**：OpenAI、通义千问（通过配置切换）

**使用场景**：
- 文档索引时：将文档分块后转换为向量
- 检索时：将用户问题转换为向量，进行相似度搜索

**配置示例**：
```yaml
embedding:
  provider: 'local'  # local/openai/qwen
  model: 'BAAI/bge-base-zh-v1.5'  # 本地模型名称
```

### 2. LLM 模型（生成回答）

**作用**：基于检索到的文档内容，生成自然语言回答

**当前配置**：
- **必须使用外部 API**：OpenAI、通义千问、文心一言、智谱 AI 等
- **不支持本地部署**：LLM 模型通常很大（几十GB），需要 GPU，不适合在普通服务器上部署

**使用场景**：
- 用户提问时：基于检索到的文档片段，生成回答
- 流式输出：支持实时流式返回回答内容

**配置示例**：
```yaml
llm:
  provider: 'qwen'  # openai/qwen/wenxin/zhipu/custom
  qwen:
    api_key: 'sk-...'
    model: 'qwen-plus'
```

---

## 三、本地模型 vs 外部大模型对比

### 1. Embedding 模型对比

| 特性 | 本地模型 | 外部 API |
|------|----------|----------|
| **模型示例** | BGE-base-zh-v1.5 | OpenAI text-embedding-3-small |
| **部署方式** | 本地运行 | API 调用 |
| **数据隐私** | ✅ 完全本地，数据不出服务器 | ❌ 数据需要发送到外部服务 |
| **成本** | ✅ 免费（一次性下载） | ❌ 按调用次数收费 |
| **速度** | ✅ 本地调用，速度快 | ⚠️ 网络延迟，速度较慢 |
| **稳定性** | ✅ 不依赖网络，稳定 | ⚠️ 依赖网络和 API 服务 |
| **资源占用** | ⚠️ 需要内存（1-2GB） | ✅ 无需本地资源 |
| **准确度** | ✅ 针对中文优化，准确度高 | ✅ 通用模型，准确度好 |
| **适用场景** | 生产环境，数据敏感 | 测试环境，资源受限 |

**推荐选择**：
- **生产环境**：推荐使用本地模型（数据隐私、成本、稳定性）
- **测试环境**：可以使用外部 API（无需下载模型）

### 2. LLM 模型对比

| 特性 | 本地部署 | 外部 API |
|------|----------|----------|
| **模型示例** | LLaMA、ChatGLM（需自行部署） | GPT-4o、Qwen Plus |
| **部署方式** | 本地服务器部署 | API 调用 |
| **数据隐私** | ✅ 完全本地，数据不出服务器 | ❌ 数据需要发送到外部服务 |
| **成本** | ⚠️ 需要 GPU 服务器（成本高） | ⚠️ 按 Token 收费 |
| **速度** | ⚠️ 取决于硬件配置 | ✅ 云端高性能，速度快 |
| **稳定性** | ⚠️ 需要维护服务器 | ✅ 服务商维护，稳定 |
| **资源占用** | ❌ 需要大量 GPU 内存（几十GB） | ✅ 无需本地资源 |
| **准确度** | ⚠️ 取决于模型选择 | ✅ 使用最新最强模型 |
| **适用场景** | 数据极度敏感，有 GPU 资源 | 大多数场景（推荐） |

**推荐选择**：
- **大多数场景**：推荐使用外部 API（成本、维护、准确度）
- **数据极度敏感**：考虑本地部署（需要 GPU 服务器）

---

## 四、本项目推荐的配置方案

### 方案 1：生产环境推荐（当前默认）

```yaml
# Embedding：本地模型
embedding:
  provider: 'local'
  model: 'BAAI/bge-base-zh-v1.5'  # 本地运行，数据隐私好

# LLM：外部 API
llm:
  provider: 'qwen'  # 或 openai
  qwen:
    api_key: 'sk-...'
    model: 'qwen-plus'
```

**优势**：
- ✅ Embedding 本地运行：数据隐私好，速度快，无成本
- ✅ LLM 使用外部 API：准确度高，无需维护，成本可控
- ✅ 最佳平衡：在隐私、成本、准确度之间取得平衡

### 方案 2：数据极度敏感场景

```yaml
# Embedding：本地模型
embedding:
  provider: 'local'
  model: 'BAAI/bge-base-zh-v1.5'

# LLM：本地部署（需要自行部署，通过 custom 配置）
llm:
  provider: 'custom'
  custom:
    api_key: ''
    base_url: 'http://localhost:8000'  # 本地部署的 LLM 服务
    model: 'local-llm'
```

**优势**：
- ✅ 完全本地化：数据不出服务器
- ⚠️ 需要自行部署 LLM 服务（需要 GPU 服务器）

### 方案 3：测试/开发环境

```yaml
# Embedding：外部 API（无需下载模型）
embedding:
  provider: 'openai'  # 或 'qwen'
  # model 参数在 API 模式下不需要

# LLM：外部 API
llm:
  provider: 'qwen'
  qwen:
    api_key: 'sk-...'
    model: 'qwen-turbo'  # 使用更便宜的模型
```

**优势**：
- ✅ 快速启动：无需下载模型
- ⚠️ 需要 API Key，有调用成本

---

## 五、为什么采用混合架构？

### 1. Embedding 使用本地模型的原因

**优势**：
- ✅ **数据隐私**：文档内容不需要发送到外部服务
- ✅ **成本**：免费使用，无调用费用
- ✅ **速度**：本地调用，无网络延迟
- ✅ **稳定性**：不依赖外部服务，更稳定
- ✅ **准确度**：可以选择针对中文优化的模型（如 BGE-base-zh-v1.5）

**为什么不用外部 API**：
- ❌ 文档索引时会产生大量 API 调用，成本高
- ❌ 文档内容需要发送到外部，隐私风险
- ❌ 网络延迟影响索引速度

### 2. LLM 使用外部 API 的原因

**优势**：
- ✅ **准确度**：可以使用最新最强的模型（GPT-4o、Qwen Plus）
- ✅ **成本**：按使用量付费，比维护 GPU 服务器便宜
- ✅ **维护**：无需维护服务器和模型
- ✅ **更新**：自动使用最新模型版本

**为什么不本地部署**：
- ❌ LLM 模型很大（几十GB），需要 GPU
- ❌ 维护成本高（服务器、电费、人力）
- ❌ 模型更新需要重新部署
- ❌ 大多数场景下，准确度不如云端最新模型

---

## 六、配置切换指南

### 切换 Embedding 模型

#### 从本地模型切换到外部 API

```yaml
embedding:
  provider: 'openai'  # 或 'qwen'
  # model 参数在 API 模式下不需要
```

**注意事项**：
- ⚠️ **需要重新索引**：向量维度可能不同（本地 768 维，OpenAI 1536 维）
- ⚠️ **需要 API Key**：配置相应的 API Key
- ⚠️ **数据隐私**：文档内容会发送到外部服务

#### 从外部 API 切换到本地模型

```yaml
embedding:
  provider: 'local'
  model: 'BAAI/bge-base-zh-v1.5'
```

**注意事项**：
- ⚠️ **需要重新索引**：向量维度可能不同
- ✅ **首次运行会自动下载模型**（约 390MB）
- ✅ **数据隐私更好**：文档内容不出服务器

### 切换 LLM 模型

#### 切换不同的 LLM 提供商

```yaml
llm:
  provider: 'qwen'  # 改为 openai/wenxin/zhipu/custom
  qwen:
    api_key: 'sk-...'
    model: 'qwen-plus'
```

**注意事项**：
- ✅ **无需重新索引**：LLM 切换不影响向量索引
- ⚠️ **需要配置 API Key**：不同提供商需要不同的认证方式
- ⚠️ **回答风格可能不同**：不同模型的回答风格可能略有差异

---

## 七、性能与成本分析

### Embedding 模型

**本地模型（BGE-base-zh-v1.5）**：
- **成本**：免费（一次性下载约 390MB）
- **速度**：本地调用，约 10-50ms/次
- **资源占用**：内存约 1-2GB
- **适用场景**：生产环境，大量文档索引

**外部 API（OpenAI）**：
- **成本**：约 $0.02 / 1M tokens（文档索引时会产生大量调用）
- **速度**：网络延迟 + API 处理，约 100-500ms/次
- **资源占用**：无需本地资源
- **适用场景**：测试环境，少量文档

### LLM 模型

**外部 API（Qwen Plus）**：
- **成本**：约 ¥0.008 / 1K tokens（输入 + 输出）
- **速度**：云端高性能，约 1-3 秒/次
- **资源占用**：无需本地资源
- **适用场景**：大多数场景

**本地部署（如 ChatGLM）**：
- **成本**：GPU 服务器（约 ¥500-2000/月）+ 电费
- **速度**：取决于硬件，通常较慢（5-10 秒/次）
- **资源占用**：需要 GPU（至少 16GB 显存）
- **适用场景**：数据极度敏感，有 GPU 资源

---

## 八、最佳实践建议

### 1. 生产环境配置

```yaml
# 推荐配置
embedding:
  provider: 'local'
  model: 'BAAI/bge-base-zh-v1.5'  # 本地模型，数据隐私好

llm:
  provider: 'qwen'  # 或 openai
  qwen:
    api_key: 'sk-...'
    model: 'qwen-plus'  # 高质量模型
```

**理由**：
- Embedding 本地运行：保护数据隐私，降低成本
- LLM 使用外部 API：获得最佳准确度，成本可控

### 2. 数据敏感场景

```yaml
embedding:
  provider: 'local'
  model: 'BAAI/bge-base-zh-v1.5'

llm:
  provider: 'custom'  # 本地部署的 LLM
  custom:
    base_url: 'http://localhost:8000'
    model: 'local-llm'
```

**理由**：
- 完全本地化，数据不出服务器
- 需要自行部署 LLM 服务

### 3. 成本优化场景

```yaml
embedding:
  provider: 'local'  # 本地模型免费
  model: 'BAAI/bge-base-zh-v1.5'

llm:
  provider: 'qwen'
  qwen:
    api_key: 'sk-...'
    model: 'qwen-turbo'  # 使用更便宜的模型
```

**理由**：
- Embedding 本地运行，无成本
- LLM 使用更便宜的模型，降低成本

---

## 九、常见问题

### Q1: 为什么 Embedding 推荐本地模型，而 LLM 推荐外部 API？

**A**: 
- **Embedding**：调用频率高（每个文档、每个问题都要调用），本地模型可以大幅降低成本，且数据隐私更好
- **LLM**：调用频率低（只有用户提问时调用），外部 API 成本可控，且可以使用最新最强的模型

### Q2: 可以完全使用本地模型吗？

**A**: 
- **Embedding**：✅ 可以，推荐使用本地模型
- **LLM**：⚠️ 可以，但需要自行部署（需要 GPU 服务器），成本和维护成本高

### Q3: 可以完全使用外部 API 吗？

**A**: 
- **Embedding**：⚠️ 可以，但成本高（文档索引时会产生大量调用），且数据隐私有风险
- **LLM**：✅ 可以，推荐使用外部 API

### Q4: 切换模型后需要重新索引吗？

**A**: 
- **Embedding 模型切换**：✅ **必须重新索引**（向量维度可能不同）
- **LLM 模型切换**：❌ 不需要重新索引（不影响向量索引）

### Q5: 如何选择 Embedding 模型？

**A**: 参考 `docs/embedding-models-comparison.md`，根据以下因素选择：
- **准确度要求**：BGE-M3 > BGE-base > BGE-small
- **资源限制**：BGE-small < BGE-base < BGE-M3
- **中英文混合**：BGE-M3 > BGE-base > BGE-small

---

## 十、总结

### 核心原则

1. **Embedding 本地化**：保护数据隐私，降低成本，提升速度
2. **LLM 云端化**：获得最佳准确度，降低维护成本
3. **混合架构**：在隐私、成本、准确度之间取得最佳平衡

### 推荐配置

```yaml
# 生产环境推荐配置
embedding:
  provider: 'local'
  model: 'BAAI/bge-base-zh-v1.5'

llm:
  provider: 'qwen'  # 或 openai
  qwen:
    api_key: 'sk-...'
    model: 'qwen-plus'
```

这种配置在**数据隐私、成本控制、回答准确度**之间取得了最佳平衡，适合大多数生产环境使用。

